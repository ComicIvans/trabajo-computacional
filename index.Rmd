---
title: "Trabajo final de Estadística Computacional"
author: "Pedro Alumbreros Menchén, Elena Merelo Molina, Iván Salido Cobo y Rubén Vílchez Valenzuela"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
  code_folding: show
self_contained: true
thumbnails: false
lightbox: true
downcute_theme: "chaos"
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{Trabajo final de Estadística Computacional}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# Análisis del texto de distintas obras de prosa, poesía y teatro del Siglo de Oro español
El objetivo es analizar las principales diferencias entre las obras más famosas de prosa, poesía y teatro escritas en España durante el siglo XVII.

Las obras escogidas son:

- **Prosa:**
  - El ingenioso hidalgo Don Quijote de la Mancha *(Miguel de Cervantes)*
  - El ingenioso caballero Don Quijote de la Mancha *(Miguel de Cervantes)*
- **Poesía:**
  - El Parnaso Español *(Francisco Gómez de Quevedo)*
  - Soledades *(Luis de Góngora)*
- **Teatro:**
  - La vida es sueño *(Pedro Calderón de la Barca)*
  - El perro del hortelano *(Félix Lope de Vega)*

Además, se utilizará un pequeño repertorio de poemas de Quevedo para mostrar el uso de los paquetes.

## Paquetes utilizados

Como las obras están en PDF, se hará uso del paquete `pdftools` para extraer el texto.

Después, se usarán los paquetes `tokenizers` y `stopwords` para descomponer el texto y filtrar ciertas palabras.

Por último, se analizarán los datos resultantes con `tidyverse`, una librería que incluye varios paquetess.

```{r }
options(max.print = 20) # Para evitar que se impriman los libros enteros.
library(pdftools)
library(tokenizers)
library(stopwords)
library(tidyverse)
```

## Leyendo los PDFs

El paguete `pdftools` permite leer el texto de un PDF con la función `pdf_text`. Se continúa leyendo el texto de todos los PDFs utilizando `lapply`. Por último, se almacenan los poemas de prueba.

```{r }
pdfs <- list.files(pattern = "pdf$")
libros <- lapply(pdfs, pdf_text)
poemas_prueba <- read_file("poemas-prueba.txt")
```

Donde `libros` es una lista con los PDFs. Cada elemento de la lista *(cada libro)*
es un vector con tantos elementos como páginas tenga el PDF y en cada elemento
el texto que haya en esa página del PDF.

Esto, sin embargo, es un inconveniente, ya que para analizar correctamente cada libro, no debería estar el texto separado por páginas, sino todo junto.
Para solucionarlo, añadiremos los datos a un *data frame* y juntaremos el texto de todas las páginas.

```{r }
libros <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(libros) <- c("titulo", "autor", "texto")
for (pdf in pdfs) {
  info <- pdf_info(pdf)
  libros[nrow(libros) + 1,] <- c(info$keys$Title, info$keys$Author, str_c(pdf_text(pdf), collapse = ""))
}

cat(strtrim(libros$texto[1], 950)) # Mostramos solo una parte de la obra.
```

## Preparando los datos

Ahora es necesario *tokenizar* el texto leído.

El proceso de *tokenización* consiste en descomponer una cadena de texto en *tokens*, esto es, elementos más pequeños computables. Un ejemplo sería la descomposición del texto en palabras o en letras.
Por otro lado, se usarán herramientas para procesar textos en distintos formatos con el fin de facilitar su manipulación.

Con la función `tokenize_words` se extraen y se almacenan las palabras.

```{r }
tokenize_words(poemas_prueba)
```

A la hora de extraer las palabras, podemos además especificar un vector de palabras vacías *(palabras que no tienen un significado por sí solas, como pueden ser artículos, pronombres, preposiciones, adverbios, ...)*, que se omitirán.
Para esto es útil el paquete `stopwords`, que contiene las palabras vacías o *stopwords* para distintos idiomas. El idioma se especifica como argumento en forma de su código ISO 639-1, por ejemplo, para el español, se usa el código `"es"`. Un ejemplo donde extraemos las palabras de un libro especificando palabras vacías del español con ayuda del paquete `stopwords`:

```{r }
tokenize_words(poemas_prueba, stopwords = stopwords::stopwords("es"))
```

Otra opción a la hora de analizar las palabras de un texto es tomar las raíces de las palabras en lugar de las palabras completas, lo cual da pie a más formas de estudiar un texto desde el punto de vista lingüístico. Esto se puede conseguir con ayuda de la función `tokenize_word_stems`:

```{r }
tokenize_word_stems(poemas_prueba, language = "spanish")
```

No solo nos quedamos en las palabras, si no que además podemos tomar el texto y separarlo en todas las letras que lo componen, que son los menores *tokens* que podremos encontrar.

```{r }
tokenize_characters(poemas_prueba)
```

En el paquete encontramos otra función que permite dividir el texto en partes más pequeñas de igual longitud.

```{r }
chunk_text(poemas_prueba, chunk_size = 80)
```

También se pueden separar párrafos y frases con las funciones `tokenize_sentences` y `tokenize_paragraphs` respectivamente. Vemos un ejemplo de separación de frases. 

```{r }
tokenize_sentences(poemas_prueba)
```

Además se pueden contar letras, palabras o frases de un texto con las funciones `count_words`, `count_characters` y `count_sentences` respectivamente.
Aunque es posible *tokenizar* el texto con las funciones ya vistas y computar la longitud de los vectores obtenidos, estas funciones nos ofrecen un recuento de forma directa si no queremos dar pasos intermedios.

```{r }
count_words(poemas_prueba)
```

Otra opción a la hora de *tokenizar* el texto, igual que tomar frases, palabras, o párrafos, es tomar *n-gramas*: subsecuencias de a lo sumo *n* elementos contiguos de una secuencia dada, y como mínimo *n_min* elementos. En este caso es también posible utilizar la opción de excluir palabras vacías si se desea. Veamos un ejemplo donde tomamos *n-gramas* de entre *1* y *6* palabras:

```{r }
tokenize_ngrams(poemas_prueba, n = 6, n_min = 1)
```

Veamos el mismo caso omitiendo palabras vacías:

```{r }
tokenize_ngrams(poemas_prueba, n = 6, n_min = 1, stopwords = stopwords::stopwords("es"))
```

A raíz de esto surge también la opción de tomar *n-gramas* con *k-saltos*: como su propio nombre indica, hace lo mismo que el proceso de tomar *n-gramas*, pero con la opción de omitir entre *0* y *k* palabras en el *n-grama*, en lugar de tomar subsecuencias con todas las palabras contiguas. De nuevo, es posible incluir una lista de palabras vacías que evitar en el proceso.

```{r }
tokenize_skip_ngrams(poemas_prueba, n = 3, n_min = 1, k = 1)
tokenize_skip_ngrams(poemas_prueba, n = 3, n_min = 1, k = 1, stopwords = stopwords::stopwords("es"))
```

Para el este estudio, se usarán como *tokens* todas las palabras eliminando las palabras vacías. Además, se eliminarán también los nombres de los personajes de las obras de teatro, que se han colocado en un documento de texto.

Una cosa a tener en cuenta es que `tokenize_words` devuelve una lista con las palabras por documento *tokenizado*. Como solo estamos *tokenizando* un único texto cada vez que usamos la función, hay que poner `[[1]]` para sacar el texto de la lista.

```{r }
personajes <- tokenize_words(read_file("personajes.txt"))[[1]]

for (i in 1:nrow(libros)) {
  libros[i, "texto"] <- str_c(tokenize_words(libros[i, "texto"], stopwords = c(stopwords::stopwords("es"), personajes))[[1]], collapse = " ")
}
```

Lo que hemos hecho es juntar todos los *tokens* en un solo *string* separados por espacios.

Para poder analizar la obras con mayor comodidad, vamos a separar cada *token* en una fila del *data frame*. La función `separate_rows` nos permite sacar todos los *tokens* separados por espacios a una fila individual.

```{r }
libros <- separate_rows(libros, texto, sep = "[[:space:]]")
head(libros)
```

Lo que nos ha devuelto `head(libros)` es las primeras filas. Observamos que `libros` ya no es un *data frame*, sino un objeto llamado `tibble`,
esto es una alternativa al *data frame* con la que trabajan los paquetes de `tidyverse`. La función `separate_rows` ha convertido automáticamente
el *data frame* a `tibble`.

## Analizando los datos

Pasamos a ver qué palabras se repiten más en los textos.

```{r }
libros %>% 
  count(texto, sort = TRUE)
```

Visualizemos la palabra más frecuente por título. Usamos `slice` para *“cortar”* una porción de los datos de cada grupo. En este caso, sólo las primeras filas, que por el orden establecido en el paso anterior, son las que tienen las palabras más frecuentes:

```{r }
libros %>% 
  filter(!is.na(titulo)) %>%
  group_by(titulo) %>%
  count(texto, sort = TRUE) %>%
  slice(5) %>%
  ggplot() +
  geom_bar(aes(x = texto, weight = n, fill = texto)) +
  coord_flip() +
  facet_wrap(~titulo) +
  labs(title = "Obras de literatura",
       subtitle = "Palabra más frecuente por obra",
       x = "palabra",
       y = "veces hallada")
```

