---
title: "Trabajo final de Estadística Computacional"
author: "Pedro Alumbreros Menchén, Elena Merelo Molina, Iván Salido Cobo y Rubén Vílchez Valenzuela"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
  code_folding: show
self_contained: true
thumbnails: false
lightbox: true
downcute_theme: "chaos"
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{Trabajo final de Estadística Computacional}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# Análisis del texto de distintas obras de prosa, poesía y teatro del Siglo de Oro español
El objetivo es analizar las principales diferencias entre las obras más famosas de prosa, poesía y teatro escritas en España durante el siglo XVII.

Las obras escogidas son:

- **Prosa:**
  - El ingenioso hidalgo Don Quijote de la Mancha *(Miguel de Cervantes)*
  - El ingenioso caballero Don Quijote de la Mancha *(Miguel de Cervantes)*
- **Poesía:**
  - El Parnaso Español *(Francisco Gómez de Quevedo)*
  - Soledades *(Luis de Góngora)*
- **Teatro:**
  - La vida es sueño *(Pedro Calderón de la Barca)*
  - El perro del hortelano *(Félix Lope de Vega)*

Además, se utilizará un pequeño repertorio de poemas de Quevedo para mostrar el uso de los paquetes.

## Paquetes utilizados

Como las obras están en PDF, se hará uso del paquete `pdftools` para extraer el texto.

Después, se usarán los paquetes `tokenizers` y `stopwords` para descomponer el texto y filtrar ciertas palabras.

Por último, se analizarán los datos resultantes con `tidyverse`, una librería que incluye varios paquetess.

```{r }
options(max.print = 20) # Para evitar que se impriman los libros enteros.
library(pdftools)
library(tokenizers)
library(stopwords)
library(tidyverse)
```

## Leyendo los PDFs

El paguete `pdftools` permite leer el texto de un PDF con la función `pdf_text`. Se continúa leyendo el texto de todos los PDFs utilizando `lapply`. Por último, se almacenan los poemas de prueba.

```{r }
pdfs <- list.files(pattern = "pdf$")
libros <- lapply(pdfs, pdf_text)
poemas_prueba <- read_file("poemas-prueba.txt")
```

Donde `libros` es una lista con los PDFs. Cada elemento de la lista *(cada libro)*
es un vector con tantos elementos como páginas tenga el PDF y en cada elemento
el texto que haya en esa página del PDF.

Esto, sin embargo, es un inconveniente, ya que para analizar correctamente cada libro, no debería estar el texto separado por páginas, sino todo junto.
Para solucionarlo, añadiremos los datos a un *data frame* y juntaremos el texto de todas las páginas.

```{r }
libros <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(libros) <- c("titulo", "autor", "texto", "n_palabras")
for (pdf in pdfs) {
  info <- pdf_info(pdf)
  libros[nrow(libros) + 1,] <- c(info$keys$Title, info$keys$Author, str_c(pdf_text(pdf), collapse = ""), NA)
}

cat(strtrim(libros$texto[1], 950)) # Mostramos solo una parte de la obra.
```

## Preparando los datos

Ahora es necesario *tokenizar* el texto leído.

El proceso de *tokenización* consiste en descomponer una cadena de texto en *tokens*, esto es, elementos más pequeños computables. Un ejemplo sería la descomposición del texto en palabras o en letras.
Por otro lado, se usarán herramientas para procesar textos en distintos formatos con el fin de facilitar su manipulación.

Con la función `tokenize_words` se extraen y se almacenan las palabras.

```{r }
tokenize_words(poemas_prueba)
```

A la hora de extraer las palabras, podemos además especificar un vector de palabras vacías *(palabras que no tienen un significado por sí solas, como pueden ser artículos, pronombres, preposiciones, adverbios, ...)*, que se omitirán.
Para esto es útil el paquete `stopwords`, que contiene las palabras vacías o *stopwords* para distintos idiomas. El idioma se especifica como argumento en forma de su código ISO 639-1, por ejemplo, para el español, se usa el código `"es"`. Un ejemplo donde extraemos las palabras de un libro especificando palabras vacías del español con ayuda del paquete `stopwords`:

```{r }
tokenize_words(poemas_prueba, stopwords = stopwords::stopwords("es"))
```

Otra opción a la hora de analizar las palabras de un texto es tomar las raíces de las palabras en lugar de las palabras completas, lo cual da pie a más formas de estudiar un texto desde el punto de vista lingüístico. Esto se puede conseguir con ayuda de la función `tokenize_word_stems`:

```{r }
tokenize_word_stems(poemas_prueba, language = "spanish")
```

No solo nos quedamos en las palabras, si no que además podemos tomar el texto y separarlo en todas las letras que lo componen, que son los menores *tokens* que podremos encontrar.

```{r }
tokenize_characters(poemas_prueba)
```

En el paquete encontramos otra función que permite dividir el texto en partes más pequeñas de igual longitud.

```{r }
chunk_text(poemas_prueba, chunk_size = 80)
```

También se pueden separar párrafos y frases con las funciones `tokenize_sentences` y `tokenize_paragraphs` respectivamente. Vemos un ejemplo de separación de frases. 

```{r }
tokenize_sentences(poemas_prueba)
```

Además se pueden contar letras, palabras o frases de un texto con las funciones `count_words`, `count_characters` y `count_sentences` respectivamente.
Aunque es posible *tokenizar* el texto con las funciones ya vistas y computar la longitud de los vectores obtenidos, estas funciones nos ofrecen un recuento de forma directa si no queremos dar pasos intermedios.

```{r }
count_words(poemas_prueba)
```

Otra opción a la hora de *tokenizar* el texto, igual que tomar frases, palabras, o párrafos, es tomar *n-gramas*: subsecuencias de a lo sumo *n* elementos contiguos de una secuencia dada, y como mínimo *n_min* elementos. En este caso es también posible utilizar la opción de excluir palabras vacías si se desea. Veamos un ejemplo donde tomamos *n-gramas* de entre *1* y *6* palabras:

```{r }
tokenize_ngrams(poemas_prueba, n = 6, n_min = 1)
```

Veamos el mismo caso omitiendo palabras vacías:

```{r }
tokenize_ngrams(poemas_prueba, n = 6, n_min = 1, stopwords = stopwords::stopwords("es"))
```

A raíz de esto surge también la opción de tomar *n-gramas* con *k-saltos*: como su propio nombre indica, hace lo mismo que el proceso de tomar *n-gramas*, pero con la opción de omitir entre *0* y *k* palabras en el *n-grama*, en lugar de tomar subsecuencias con todas las palabras contiguas. De nuevo, es posible incluir una lista de palabras vacías que evitar en el proceso.

```{r }
tokenize_skip_ngrams(poemas_prueba, n = 3, n_min = 1, k = 1)
tokenize_skip_ngrams(poemas_prueba, n = 3, n_min = 1, k = 1, stopwords = stopwords::stopwords("es"))
```

Para el este estudio, se usarán como *tokens* todas las palabras eliminando las palabras vacías. Además, se eliminarán también los nombres de los personajes de las obras y algunas palabras más, que se han colocado en un documento de texto.

Después de *tokenizar* el texto, se guardará también la cantidad de palabras que quedan.

Una cosa a tener en cuenta es que `tokenize_words` devuelve una lista con las palabras por documento *tokenizado*. Como solo estamos *tokenizando* un único texto cada vez que usamos la función, hay que poner `[[1]]` para sacar el texto de la lista.

```{r }
palabras_extra <- tokenize_words(read_file("palabras_extra.txt"))[[1]]

for (i in 1:nrow(libros)) {
  libros[i, "texto"] <- str_c(tokenize_words(libros[i, "texto"], stopwords = c(stopwords::stopwords("es"), palabras_extra))[[1]], collapse = " ")
  libros[i, "n_palabras"] <- count_words(libros[i, "texto"])
}
```

Lo que hemos hecho es juntar todos los *tokens* en un solo *string* separados por espacios.

Para poder analizar la obras con mayor comodidad, vamos a separar cada *token* en una fila del *data frame*. La función `separate_rows` nos permite sacar todos los *tokens* separados por espacios a una fila individual.

```{r }
libros_separados <- separate_rows(libros, texto, sep = "[[:space:]]")
head(libros_separados)
```

Lo que nos ha devuelto `head(libros)` es las primeras filas. Observamos que `libros` ya no es un *data frame*, sino un objeto llamado `tibble`,
esto es una alternativa al *data frame* con la que trabajan los paquetes de `tidyverse`. La función `separate_rows` ha convertido automáticamente
el *data frame* a `tibble`.

## Analizando los datos

Pasamos a ver qué palabras se repiten más en los textos. Aquí vamos a utilizar el operador `%>%`, que le pasa la parte de la izquierda a la función de la derecha como primer parámetro.

```{r }
palabras_repetidas <- libros_separados %>% 
  count(texto, sort = TRUE)

palabras_repetidas
```

Para mostrar los datos en un gráfico, utilizaremos la función `ggplot`. A esa función hay que proporcionarle los datos a mostrar. Pedirle que muestre todas las palabras sería una locura, así que se mostrarán solo unas pocas. 

Usamos `slice_head` para *“cortar”* una porción de los datos de cada grupo. En este caso, sólo las primeras filas, que son las que tienen las palabras más frecuentes:

La función `ggplot` recibe también el tipo de gráfico, que será `geom_bar`, es decir, un gráfico de barras. `aes` indica la variable que se utilizará para el gráfico.

Por último `coord_flip` invierte las coordenadas para que las barras sean horizontales y `labs` genera la leyenda.

```{r }
palabras_repetidas %>%  
  slice_head(n = 20) %>%
  ggplot() +
  geom_bar(aes(x = texto, weight = n, fill = texto)) +
  coord_flip() +
  labs(title = "Obras de literatura",
       subtitle = "Palabras más frecuentes",
       x = "palabra",
       y = "veces hallada")
```

Visualizemos las palabras más frecuente por título.

```{r }
libros_separados %>% 
  group_by(titulo) %>%
  count(texto, sort = TRUE) %>%
  slice_head(n = 5) %>%
  ggplot() +
  geom_bar(aes(x = texto, weight = n, fill = texto)) +
  coord_flip() +
  facet_wrap(~titulo) +
  labs(title = "Obras de literatura",
     subtitle = "Palabras más frecuentes por obra",
     x = "palabra",
     y = "veces hallada")
```

Inmediatamente nos damos cuenta de un problema, en las obras del Quijote hay muchas más repeticiones porque la obra es más extensa, no necesariamente porque se repitan más veces esas palabras.

Para solucionarlo, podemos intentar ver en qué proporción se repiten esas palabras con respecto al total de palabras que tiene el libro.

La función `count` nos permite indicar unos pesos para cada observación, estos pesos serán `1 / n_palabras`, lo que nos dará la proporción de veces que aparece la palabra con el total de palabras que tiene la obra.

```{r }
libros_separados %>% 
  group_by(titulo) %>%
  count(texto, sort = TRUE, wt = 1 / as.numeric(n_palabras)) %>%
  slice_head(n = 5) %>%
  ggplot() +
  geom_bar(aes(x = texto, weight = n, fill = texto)) +
  coord_flip() +
  facet_wrap(~titulo) +
  labs(title = "Obras de literatura",
       subtitle = "Proporción de palabras más frecuentes por obra",
       x = "palabra",
       y = "proporción hallada")
```

Vovemos a fijarnos en algo extraño, y es que no parece que se hayan contado las palabras bien en algunas obras. Por ejemplo, la gráfica nos dice que en ninguno de los libros del Quijote aparece la palabra *"sol"*, esto no tiene sentido, pues si buscamos manualmente en el documento veremos que sí aparece.

Para entender el problema hay que analizar los datos que le estamos pasando a la gráfica para mostrar.

```{r }
libros_separados %>% 
  group_by(titulo) %>%
  count(texto, sort = TRUE)
```

Comparamos la diferencia de contar las palabras habiendo agrupado por título y sin haberlo hecho *(que tenemos un poco más arriba en la variable `palabras repetidas`)*.

Entonces, lo que hace `group_by` es que las palabras se cuenten por libro, no en general.

Veamos nuevamente cómo quedan ordeandas las palabras con los pesos, ya que nos interesa ver qué palabra aparece más en proporción, no simplemente porque haya obras más largas que otras.

```{r }
libros_separados %>% 
  count(texto, sort = TRUE, wt = 1 / as.numeric(n_palabras))
```

Comparamos el resultado agrupando por título.

```{r }
libros_separados %>% 
  group_by(titulo) %>%
  count(texto, sort = TRUE, wt = 1 / as.numeric(n_palabras))
```

Vemos que, aplicando la proporción, no salen las mismas palabras que sin aplicarla. Nos queda comprobar qué hacen `slice` y `slice_head`.

```{r }
libros_separados %>% 
  count(texto, sort = TRUE, wt = 1 / as.numeric(n_palabras)) %>%
  slice_head(n = 5) # `slice_head` necesita especificar `n = 5`, `slice` permite poner el número directamente.

libros_separados %>% 
  count(texto, sort = TRUE, wt = 1 / as.numeric(n_palabras)) %>%
  slice(5)
```

Así que `slice_head` nos devuelve los primeros *n* elementos, mientas que `slice` nos devuelve el elemento en la posición `n`. Para ver las principales palabras más repetidas nos interesará utilizar `slice_head`.

Por último, veamos código completo que se le pasa a la gráfica.

```{r }
libros_separados %>% 
  group_by(titulo) %>%
  count(texto, sort = TRUE, wt = 1 / as.numeric(n_palabras)) %>%
  slice_head(n = 5)
```

Y ahí está el problema, `slice_head` nos devuelve las *5* primeras palabras más repetidas en cada libro.
Como no siempre son las mismas en cada libro, hay gráficas en las que no aparece la cantidad de repeticiones de una palabra porque no está entre las *5* más repetidas de ese libro.

¿Cuál sería la solución? Primero hay que tener claro qué queremos mostrar. Queremos ver cuánto aparecen en cada libro las palabras más repetidas en general, es decir, buscar en cada libro las repeticiones de las *n* primeras palabras de `palabras_repetidas`:

```{r }
palabras_buscadas <- palabras_repetidas %>% slice_head(n = 10)
```

Vamos a intentar conseguir esas palabras en cada libro.

```{r }
libros_separados %>% 
  group_by(titulo) %>%
  count(texto, sort = TRUE, wt = 1 / as.numeric(n_palabras)) %>%
  filter(texto %in% palabras_buscadas$texto) %>%
  ggplot() +
  geom_bar(aes(x = texto, weight = n, fill = texto)) +
  coord_flip() +
  facet_wrap(~titulo) +
  labs(title = "Obras de literatura",
       subtitle = "Proporción de palabras más frecuentes por obra",
       x = "palabra",
       y = "proporción hallada")
```

¡Y ahora sí que tienen mucho más sentido los resultados!

Aun así, las palabras que aparecen como más repetitivas no tienen mucho interés, vamos a probar a coger palabras que estén entre las más repetidas pero sean las primeras. Aquí si usaremos `slice`:

```{r }
palabras_buscadas <- palabras_repetidas %>% slice(10:20)

libros_separados %>% 
  group_by(titulo) %>%
  count(texto, sort = TRUE, wt = 1 / as.numeric(n_palabras)) %>%
  filter(texto %in% palabras_buscadas$texto) %>%
  ggplot() +
  geom_bar(aes(x = texto, weight = n, fill = texto)) +
  coord_flip() +
  facet_wrap(~titulo) +
  labs(title = "Obras de literatura",
       subtitle = "Proporción de palabras más frecuentes por obra",
       x = "palabra",
       y = "proporción hallada")
```

Como curiosidad, podemos ver las palabras menos repetidas en vez de las más repetidas con `slice_tail`:

```{r }
palabras_buscadas <- palabras_repetidas %>% slice_tail(n = 15)

libros_separados %>% 
  group_by(titulo) %>%
  count(texto, sort = TRUE, wt = 1 / as.numeric(n_palabras)) %>%
  filter(texto %in% palabras_buscadas$texto) %>%
  ggplot() +
  geom_bar(aes(x = texto, weight = n, fill = texto)) +
  coord_flip() +
  facet_wrap(~titulo) +
  labs(title = "Obras de literatura",
       subtitle = "Proporción de palabras más frecuentes por obra",
       x = "palabra",
       y = "proporción hallada")
```

Curiosamente, todas las palabras menos repetidas empiezan por *"z"*.
